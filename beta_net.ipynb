{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Beta Model!\n","\n","- This current working model was only trained to 1 epoch. This file will reflect the attempt made to train it up to 50 epochs ... which was a mistake as the google colab server timed out, despite having google colab +. \n","\n","- The model was trained and compiled on a TPU in google colab. \n","\n","- The working model and all its outputs are available in the repo. **Links are provided in this file within their respective sections of the code.**\n","\n","- The next step is to build a web integration to deploy the model AND to integrate image augmentation techniques. For this week, the focus is to complete the file to deploy the model. \n"]},{"cell_type":"markdown","metadata":{},"source":["### PLEASE HELP ME FIGURE OUT THE IMAGE AUGMENTATION SECTION. I have included links to the tutorials I have tried to follow as well as its resources. "]},{"cell_type":"markdown","metadata":{"id":"6kCq1woUeuv_"},"source":["### Import Dependencies And Define Paths"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"f42f6560-edf0-4efb-85a6-6e945e50895b","_uuid":"3300a1edbf2e8122d88093998eb503a6fab8a719","collapsed":true,"executionInfo":{"elapsed":3072,"status":"ok","timestamp":1658442211178,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"0a61uOx4euwC"},"outputs":[],"source":["import numpy as np \n","import pandas as pd  \n","# %matplotlib inline\n","import matplotlib.pyplot as plt\n","from itertools import chain\n","from keras.preprocessing.image import ImageDataGenerator \n","from sklearn.metrics import roc_curve, auc\n","import keras\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20213,"status":"ok","timestamp":1658442231386,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"gHmpdvZNewlC","outputId":"c46c8552-3f52-4962-e03d-d00ca1d1f743"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Mount your google drive to access the files \n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1031,"status":"ok","timestamp":1658442551110,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"1TWuBnKmfB4l"},"outputs":[],"source":["# Import Data Files \n","\n","rtrain =pd.read_csv(\"/content/drive/MyDrive/NIH-CXR/train_set.csv\")\n","rtest = pd.read_csv(\"/content/drive/MyDrive/NIH-CXR/test_set.csv\")\n","rval = pd.read_csv(\"/content/drive/MyDrive/NIH-CXR/val_set.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["### Clean the Dataframes and extract labels "]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":144,"status":"ok","timestamp":1658442552182,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"H8VrA6odY565"},"outputs":[],"source":["# Fixes the paths to locate files in my google drive\n","\n","def path_fix(df, column, header):\n","  copy_df = df.copy()\n","  # Insert the header path for your drive folder\n","  copy_df[column] = header + copy_df[column].astype(str)\n","  # Replace the backslashes with forward slashes as a string. \\\\\\\\ is for regex purposes\n","  copy_df[column] = copy_df[column].str.replace(\"\\\\\\\\\", \"/\")\n","    \n","  return copy_df"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":198,"status":"ok","timestamp":1658442554212,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"SqspPryaeuwE"},"outputs":[],"source":["# Function to clean the sets and extract the labels. \n","\n","def label_cleaner(raw_df):\n","    # Copy the Df into two separate dataframes, one for saving as csv and one for output\n","    copy_df = raw_df.copy()\n","\n","    # Create a column for categorical labels list in case of use\n","    copy_df[\"Labels\"] = copy_df[\"Finding Labels\"].replace(\"No Finding\", \"\").str.split(\"|\", expand = False)\n","    # Get a list of all diseases \n","    di = list(chain(*copy_df[\"Finding Labels\"].replace(\"No Finding\", \"\").str.split(\"|\", expand = False).tolist()))\n","    # Drop unnecessary columns from df \n","    copy_df.drop(columns = ['Unnamed: 0', 'og_idx'], axis= 1, inplace= True )\n","\n","    #Extract the list of diseases as a regualr list and as an numpy array because the model likes arrays    \n","    ls = np.unique(di).tolist()\n","    ls.remove('')\n","    np_ls = np.array(ls)\n","\n","    return copy_df, ls, np_ls\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":109,"status":"ok","timestamp":1658442555944,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"Dc-BhfKegdPi"},"outputs":[],"source":["# Define constants for the column name and the path header. \n","\n","COL = \"path\"\n","HEAD = \"/content/drive/MyDrive/NIH-CXR/\""]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":409},"executionInfo":{"elapsed":346,"status":"ok","timestamp":1658442557343,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"OgfmsryDguFq","outputId":"3594bf3d-c4ba-4504-cf48-60a102b3a582"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n","  \n"]},{"data":{"text/html":["\n","  <div id=\"df-130b396a-6655-42a6-ab1d-5f3cab8e4964\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>og_idx</th>\n","      <th>Image Index</th>\n","      <th>Finding Labels</th>\n","      <th>path</th>\n","      <th>Atelectasis</th>\n","      <th>Cardiomegaly</th>\n","      <th>Consolidation</th>\n","      <th>Edema</th>\n","      <th>Effusion</th>\n","      <th>Emphysema</th>\n","      <th>Fibrosis</th>\n","      <th>Infiltration</th>\n","      <th>Mass</th>\n","      <th>Nodule</th>\n","      <th>Pleural_Thickening</th>\n","      <th>Pneumonia</th>\n","      <th>Pneumothorax</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>29453</td>\n","      <td>44578</td>\n","      <td>00011460_052.png</td>\n","      <td>No Finding</td>\n","      <td>/content/drive/MyDrive/NIH-CXR/images_005/imag...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>33813</td>\n","      <td>50949</td>\n","      <td>00012880_011.png</td>\n","      <td>No Finding</td>\n","      <td>/content/drive/MyDrive/NIH-CXR/images_006/imag...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>32929</td>\n","      <td>49712</td>\n","      <td>00012620_014.png</td>\n","      <td>Effusion</td>\n","      <td>/content/drive/MyDrive/NIH-CXR/images_006/imag...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11713</td>\n","      <td>17922</td>\n","      <td>00004822_026.png</td>\n","      <td>Atelectasis|Infiltration</td>\n","      <td>/content/drive/MyDrive/NIH-CXR/images_003/imag...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>76383</td>\n","      <td>111882</td>\n","      <td>00030650_007.png</td>\n","      <td>Effusion|Infiltration|Nodule|Pleural_Thickening</td>\n","      <td>/content/drive/MyDrive/NIH-CXR/images_012/imag...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-130b396a-6655-42a6-ab1d-5f3cab8e4964')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-130b396a-6655-42a6-ab1d-5f3cab8e4964 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-130b396a-6655-42a6-ab1d-5f3cab8e4964');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   Unnamed: 0  og_idx       Image Index  \\\n","0       29453   44578  00011460_052.png   \n","1       33813   50949  00012880_011.png   \n","2       32929   49712  00012620_014.png   \n","3       11713   17922  00004822_026.png   \n","4       76383  111882  00030650_007.png   \n","\n","                                    Finding Labels  \\\n","0                                       No Finding   \n","1                                       No Finding   \n","2                                         Effusion   \n","3                         Atelectasis|Infiltration   \n","4  Effusion|Infiltration|Nodule|Pleural_Thickening   \n","\n","                                                path  Atelectasis  \\\n","0  /content/drive/MyDrive/NIH-CXR/images_005/imag...            0   \n","1  /content/drive/MyDrive/NIH-CXR/images_006/imag...            0   \n","2  /content/drive/MyDrive/NIH-CXR/images_006/imag...            0   \n","3  /content/drive/MyDrive/NIH-CXR/images_003/imag...            1   \n","4  /content/drive/MyDrive/NIH-CXR/images_012/imag...            0   \n","\n","   Cardiomegaly  Consolidation  Edema  Effusion  Emphysema  Fibrosis  \\\n","0             0              0      0         0          0         0   \n","1             0              0      0         0          0         0   \n","2             0              0      0         1          0         0   \n","3             0              0      0         0          0         0   \n","4             0              0      0         1          0         0   \n","\n","   Infiltration  Mass  Nodule  Pleural_Thickening  Pneumonia  Pneumothorax  \n","0             0     0       0                   0          0             0  \n","1             0     0       0                   0          0             0  \n","2             0     0       0                   0          0             0  \n","3             1     0       0                   0          0             0  \n","4             1     0       1                   1          0             0  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Fix the paths specific to google drive format \n","\n","ftrain = path_fix(rtrain, COL, HEAD)\n","ftest = path_fix(rtest, COL, HEAD)\n","fval = path_fix(rval, COL, HEAD)\n","\n","ftrain.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":365,"status":"ok","timestamp":1658442560356,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"q1y9woRHeuwE","outputId":"c495c870-8e73-4fcc-fbef-d6ed2f21c95f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Set Shape: (30000, 17),\n","Test Set Shape: (10000, 17),\n","Val Set Shape: (10000, 17)\n"]}],"source":["# Clean and extract labels\n","\n","train_set, labels, np_labels = label_cleaner(ftrain)\n","test_set, _, _ = label_cleaner(ftest)\n","val_set, _, _ = label_cleaner(fval)\n","\n","print(f\"Train Set Shape: {train_set.shape},\\nTest Set Shape: {test_set.shape},\\nVal Set Shape: {val_set.shape}\")"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":146,"status":"ok","timestamp":1658442562217,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"4fwqB0-GeuwF","outputId":"052ce9ac-3b39-42a6-a2f3-6600f7acb85a"},"outputs":[{"data":{"text/plain":["['Atelectasis',\n"," 'Cardiomegaly',\n"," 'Consolidation',\n"," 'Edema',\n"," 'Effusion',\n"," 'Emphysema',\n"," 'Fibrosis',\n"," 'Infiltration',\n"," 'Mass',\n"," 'Nodule',\n"," 'Pleural_Thickening',\n"," 'Pneumonia',\n"," 'Pneumothorax']"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["labels"]},{"cell_type":"markdown","metadata":{"id":"FB24Wa2yEb9w"},"source":["### In case We need to pre-process our images external to the generator, this will split the X and Y from all the sets.\n","\n","- This section would not need to be included if we are using the `ImageDataGenerator()` and the `flow_from_dataframe()` generator functions to generate batches of images. \n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1658396648378,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"vc1AUUGzEksS"},"outputs":[],"source":["# Function to split between x and y \n","\n","def xysplitter(df, xcol, ycol): \n","  copy_df = df.copy()\n","  x = copy_df[xcol]\n","  y = copy_df[ycol]\n","\n","  return x, y\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1658396648379,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"gY5F51YqHHrK"},"outputs":[],"source":["# Split the sets\n","\n","xtrain, ytrain = xysplitter(train_set, xcol = 'path', ycol = labels)\n","xtest, ytest =  xysplitter(test_set, xcol = 'path', ycol = labels)\n","xval, yval =  xysplitter(val_set, xcol = 'path', ycol = labels)"]},{"cell_type":"markdown","metadata":{"id":"iYjokmvxoznj"},"source":["### Sample Histogram Equalization (PLEASE HELP ME)\n","\n","- Please send help. Below are two tutorial links that I have tried to follow but to no avail. \n","\n","    - [Hist Eq integration to ImageDataGenerator()](https://towardsdatascience.com/image-augmentation-for-deep-learning-using-keras-and-histogram-equalization-9329f6ae5085)\n","\n","    - [Histogram Equalize RGB images external to Keras](https://towardsdatascience.com/histogram-equalization-a-simple-way-to-improve-the-contrast-of-your-image-bcd66596d815). Current Approach\n","\n","- Additionally I have found other modules' documentation that could help me perform the augmentation after integration into my current working code. \n","\n","    - [Kornia Module](https://kornia-tutorials.readthedocs.io/en/latest/image_histogram.html)\n","\n","    - [Tensor Flow add-ons](https://www.tensorflow.org/addons/api_docs/python/tfa/image/equalize)\n","\n","    - [PyTorch](https://pytorch.org/vision/main/generated/torchvision.transforms.functional.equalize.html)\n","\n","    - [PyTorch Transofrmations: Including Gaussian Blur](https://pytorch.org/vision/main/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py). Honestly, I'm probably going to settle for this? \n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1658396648379,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"iKPKmYs4oyk4"},"outputs":[],"source":["# Sample an image from the train set\n","# img = xtrain[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1658396648380,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"ndAfj_Lz8mKq"},"outputs":[],"source":["# Import Dependencies \n","import cv2 as cv\n","\n","# Function to histogram equalize images across RGB channels. The source code is the second link up top.\n","def histogram_equalization(img_in):\n","# segregate color streams\n","  b,g,r = cv.split(img_in)\n","  h_b, bin_b = np.histogram(b.flatten(), 256, [0, 256])\n","  h_g, bin_g = np.histogram(g.flatten(), 256, [0, 256])\n","  h_r, bin_r = np.histogram(r.flatten(), 256, [0, 256])\n","# calculate cdf    \n","  cdf_b = np.cumsum(h_b)  \n","  cdf_g = np.cumsum(h_g)\n","  cdf_r = np.cumsum(h_r)\n","  \n","# mask all pixels with value=0 and replace it with mean of the pixel values \n","  cdf_m_b = np.ma.masked_equal(cdf_b,0)\n","  cdf_m_b = (cdf_m_b - cdf_m_b.min())*255/(cdf_m_b.max()-cdf_m_b.min())\n","  cdf_final_b = np.ma.filled(cdf_m_b,0).astype('uint8')\n","\n","  cdf_m_g = np.ma.masked_equal(cdf_g,0)\n","  cdf_m_g = (cdf_m_g - cdf_m_g.min())*255/(cdf_m_g.max()-cdf_m_g.min())\n","  cdf_final_g = np.ma.filled(cdf_m_g,0).astype('uint8')\n","\n","  cdf_m_r = np.ma.masked_equal(cdf_r,0)\n","  cdf_m_r = (cdf_m_r - cdf_m_r.min())*255/(cdf_m_r.max()-cdf_m_r.min())\n","  cdf_final_r = np.ma.filled(cdf_m_r,0).astype('uint8')\n","# merge the images in the three channels\n","  img_b = cdf_final_b[b]\n","  img_g = cdf_final_g[g]\n","  img_r = cdf_final_r[r]\n","\n","  img_out = cv.merge((img_b, img_g, img_r))\n","  \n","  return img_out\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":148,"status":"ok","timestamp":1658396648521,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"UFDn7bBz_l_M"},"outputs":[],"source":["# imgin = cv.imread(img)\n","# out = histogram_equalization(imgin)\n","# out.dtype"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1658396648522,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"hCAvsM65ALv3"},"outputs":[],"source":["# image_decoded = tf.convert_to_tensor(out)"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1658396648522,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"wsXxeoeUBmQr"},"outputs":[],"source":["# # Read an image from a file\n","# image_string = tf.io.read_file(img)\n","# # Decode it into a dense vector\n","# image_decoded = tf.image.decode_png(image_string, channels=3)\n","\n","# image_decoded.dtype"]},{"cell_type":"markdown","metadata":{"id":"YlJ1DC71euwH"},"source":["### Image Batching: Generator Version is Commented out\n","\n","- Image data is preprocessed and fed into the model in batches by iterating through the directories where the images are located. \n","\n","- **Current version creates a custom generator that optimizes the batching process.** The source code is located [here](https://towardsdatascience.com/multi-label-image-classification-in-tensorflow-2-0-7d4cf8a4bc72)\n","\n","- The Generator version (not current) was obtained from [here](https://vijayabhaskar96.medium.com/multi-label-image-classification-tutorial-with-keras-imagedatagenerator-cd541f8eaf24)\n","\n","- The Generator version is a lot more memory intensive and slower. ***The current version does not include any augmentations.***\n","\n","- Based on the [literature](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0265949), the optimal augmentation methods are [Histogram Equalization, Guassian Blur, and (possibly)Adaptive Masking]. \n"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1658396648523,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"62zq5_ttKe2M"},"outputs":[],"source":["# Define Constants for image parameters. \n","\n","IMG_SIZE = 224 # Specify height and width of image to match the input format of the model\n","CHANNELS = 3 # Keep RGB color channels to match the input format of the model"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1658396648523,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"0BMbu-cVKtj7"},"outputs":[],"source":["# Function to read the image files and transform them into vectors\n","\n","def parse_function(filename, label):\n","    \"\"\"Function that returns a tuple of normalized image array and labels array.\n","    Args:\n","        filename: string representing path to image\n","        label: 0/1 one-dimensional array of size N_LABELS\n","    \"\"\"\n","    \n","    # Read an image from a file\n","    image_string = tf.io.read_file(filename)\n","    # Decode it into a dense vector\n","    image_decoded = tf.image.decode_png(image_string, channels=CHANNELS)\n","    \n","    \"\"\"# Read Image using the cv module \n","    image = cv.imread(str(filename))\n","    # Perform Histogram equalization across all 3 channels. \n","    equalized = histogram_equalization(image)\n","    # convert to tensors from nparray\n","    image_decoded = tf.convert_to_tensor(equalized)\n","    \"\"\"\n","    \n","    # Resize it to fixed shape\n","    image_resized = tf.image.resize(image_decoded, [IMG_SIZE, IMG_SIZE])\n","    # Normalize it from [0, 255] to [0.0, 1.0]\n","    image_normalized = image_resized / 255.0\n","    return image_normalized, label"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1658396648524,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"EEjwgqFZK8YG"},"outputs":[],"source":["# Define constants for the batching and shuffling \n","\n","BATCH_SIZE = 32 # Big enough to not crash the processor\n","AUTOTUNE = tf.data.experimental.AUTOTUNE # Adapt preprocessing and prefetching dynamically to reduce GPU and CPU idle time\n","SHUFFLE_BUFFER_SIZE = 256 # Shuffle the training data by a chunck of 256 observations"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1658396648524,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"FikQcg6hLBFj"},"outputs":[],"source":["# Function to Generate the dataset required\n","\n","def create_dataset(filenames, labels, is_training=True):\n","    \"\"\"Load and parse dataset.\n","    Args:\n","        filenames: list of image paths\n","        labels: numpy array of shape (BATCH_SIZE, N_LABELS)\n","        is_training: boolean to indicate training mode\n","    \"\"\"\n","    \n","    # Create a first dataset of file paths and labels\n","    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n","    # Parse and preprocess observations in parallel\n","    dataset = dataset.map(parse_function, num_parallel_calls=AUTOTUNE)\n","    \n","    if is_training == True:\n","        # This is a small dataset, only load it once, and keep it in memory.\n","        dataset = dataset.cache()\n","        # Shuffle the data each buffer size\n","        dataset = dataset.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n","        \n","    # Batch the data for multiple steps\n","    dataset = dataset.batch(BATCH_SIZE)\n","    # Fetch batches in the background while the model is training.\n","    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n","    \n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":138,"status":"ok","timestamp":1658396648654,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"IExrfywFMihn"},"outputs":[],"source":["# Create the DataSets\n","\n","train_ds = create_dataset(xtrain, ytrain)\n","val_ds = create_dataset(xval, yval)"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1658396648655,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"1zfUL2MxeuwH"},"outputs":[],"source":["# Define image size and instantiate an Image Data Generator and define its parameters\n","# The parameters focus on random modifications to the images to ensure the model doesn't focus on the wrong features\n","# Since the image is in greyscale we don't have to worry about messing with the parameters that deal with color/ brightness\n","# Just the physical modifications of the images themselves. \n","# We have a lot of leeway with the images since the features should be localized towards the middle most of the time.\n","# Don't go too wild with the modifications and shift out of range/ focus of the features.\n","\"\"\"\n","image_size = (224, 224)\n","\n","imdg = ImageDataGenerator(\n","    rescale = 1/255,\n","    samplewise_center = True, # Set each sample mean to 0.  \n","    samplewise_std_normalization = True, # Divide each input by its std. \n","    horizontal_flip = True, # Flips samples horizontally randomly to prevent over fitting over wrong features\n","    vertical_flip = False, \n","    width_shift_range=0.1, # Define how the images should be randomly shifted when fed, 10% of image \n","    height_shift_range= 0.05, # Define how the images should be  randomly shifted when fed, 5% of image\n","    shear_range = 0.05, # slants the images in a certain degree based on the value.\n","    zoom_range=0.05, # magnify the images randomly by 5% \n","    rotation_range= 2, # rotate the images by 2 \n","fill_mode = 'reflect' # if images have empty values due to modifications, reflect the image. \n",")\n","\"\"\""]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1658396648655,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"7sggWRileuwI"},"outputs":[],"source":["# Define the generators for each of the dataframes\n","\"\"\"\n","train_generator = imdg.flow_from_dataframe(\n","    dataframe=train_set,\n","    x_col=\"path\",\n","    y_col= np_labels,\n","    target_size=image_size,\n","    batch_size=32,\n","    class_mode='raw',\n","    shuffle=True,\n","    seed=10,\n",")\n","val_generator = imdg.flow_from_dataframe(\n","    dataframe=val_set,\n","    x_col=\"path\",\n","    y_col= np_labels,\n","    target_size=image_size,\n","    batch_size= 256,\n","    class_mode='raw', \n","    shuffle=False,\n","    seed=10,\n",")\n","\n","test_generator = imdg.flow_from_dataframe(\n","    dataframe=test_set,\n","    x_col=\"path\",\n","    target_size=image_size,\n","    batch_size= 200,\n","    class_mode= None,\n","    shuffle=False,\n","    seed=10,\n",")\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1658396648655,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"QVEtXRSleuwJ","outputId":"188f972d-b5e8-407f-bec7-ddee0490ec65"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\n# Sample the images to be fed from the training set and visuzalize them\\n# Adjust the ImageData Generator accordingly based on output variability. \\n\\nplt.style.use('dark_background')\\nt_x, t_y = next(train_generator)\\n# instantiate a figure and axes to plug images into \\nfig, m_axs = plt.subplots(4, 4, figsize = (16, 16))\\n# Define how each of the images will be plotted each image will have its label associated so importnat to zip\\n# Flatten the images to reduce dimensionality\\nfor (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\\n    # Display the image using imshow along a new axies instance across all dimensions \\n    # Define the color map for bones and xrays using cmap and let it cover over 150% using vmin/vmax \\n    c_ax.imshow(c_x[:,:,0], cmap = 'bone', vmin = -1.5, vmax = 1.5)\\n    # Loop through the labels in the target output using the labels and plot them accordingly. \\n    c_ax.set_title(', '.join([n_class for n_class, n_score in zip(labels, c_y) \\n                             if n_score>0.5]))\\n    # Turn off the axies ticks so that the images aren't cluttered. \\n    c_ax.axis('off')\\n\""]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["# Sample the images to be fed from the training set and visuzalize them\n","# This code is currently not being deployed since the current batching method does not use the generator. \n","# Retrofitting required for this code to be viable with the current set up \n","\"\"\"\n","# Adjust the ImageData Generator accordingly based on output variability. \n","\n","plt.style.use('dark_background')\n","t_x, t_y = next(train_generator)\n","# instantiate a figure and axes to plug images into \n","fig, m_axs = plt.subplots(4, 4, figsize = (16, 16))\n","# Define how each of the images will be plotted each image will have its label associated so importnat to zip\n","# Flatten the images to reduce dimensionality\n","for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n","    # Display the image using imshow along a new axies instance across all dimensions \n","    # Define the color map for bones and xrays using cmap and let it cover over 150% using vmin/vmax \n","    c_ax.imshow(c_x[:,:,0], cmap = 'bone', vmin = -1.5, vmax = 1.5)\n","    # Loop through the labels in the target output using the labels and plot them accordingly. \n","    c_ax.set_title(', '.join([n_class for n_class, n_score in zip(labels, c_y) \n","                             if n_score>0.5]))\n","    # Turn off the axies ticks so that the images aren't cluttered. \n","    c_ax.axis('off')\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"WgffqKpOeuwJ"},"source":["### Train the Model\n","\n","- The model is based off the [literature](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0265949), previously discussed. \n","\n","- The base feature extractor layer is [ResNet101](https://www.kaggle.com/datasets/pytorch/resnet101) that includes all the convolution and max pooling layers. Transfer Learning can be implemented to also train this layer to extract the most optimal features but it is already pre-trained so it is unnecessary. \n","\n","- The model was last run for 12 hours to attempt to reach 50 epochs but crashed due to google colab server timeout. This means we should only train up to 20 epochs max. Plus it did not seem like the val loss was improving anyways after about 20 epochs. \n","\n","- The working model that is currently uploaded to the repo as [`beta_model.h5`](https://github.com/emilybstevens/CXR-ML/blob/main/b_model.h5) was only trained for 1 epoch. \n"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1658396648656,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"htRbj7EXeuwJ"},"outputs":[],"source":["# Import Dependencies for the CNN\n","\n","from keras.applications.resnet import ResNet101\n","from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten\n","from keras.models import Sequential\n","from tensorflow.keras.optimizers import Adam \n","from keras.callbacks import ModelCheckpoint"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3059,"status":"ok","timestamp":1658396651710,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"lejzxpV2euwK","outputId":"b1688b08-097f-4fcd-dd1d-deb18bcf4730"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," resnet101 (Functional)      (None, 7, 7, 2048)        42658176  \n","                                                                 \n"," global_average_pooling2d (G  (None, 2048)             0         \n"," lobalAveragePooling2D)                                          \n","                                                                 \n"," flatten (Flatten)           (None, 2048)              0         \n","                                                                 \n"," dropout (Dropout)           (None, 2048)              0         \n","                                                                 \n"," hidden_layer (Dense)        (None, 512)               1049088   \n","                                                                 \n"," dropout_1 (Dropout)         (None, 512)               0         \n","                                                                 \n"," output_layer (Dense)        (None, 13)                6669      \n","                                                                 \n","=================================================================\n","Total params: 43,713,933\n","Trainable params: 1,055,757\n","Non-trainable params: 42,658,176\n","_________________________________________________________________\n"]}],"source":["# Differential Model Incorporating ResNet101\n","\n","feature_extractor_layer = ResNet101(input_shape =  (IMG_SIZE,IMG_SIZE,CHANNELS), \n","                                 include_top = False, weights = None, pooling = max)\n","\n","# trainable has to be false in order to freeze the layers\n","feature_extractor_layer.trainable = False # \n","b_model = Sequential()\n","b_model.add(feature_extractor_layer)\n","b_model.add(GlobalAveragePooling2D())\n","b_model.add(Flatten()) # Flatten out the tensor so that the drop out layer is not processing too much information. \n","b_model.add(Dropout(0.2))\n","b_model.add(Dense(512, activation= 'relu', name='hidden_layer'))\n","b_model.add(Dropout(0.5))\n","b_model.add(Dense(13, activation = 'sigmoid', name='output_layer'))\n","b_model.compile(Adam(learning_rate=1e-4),loss=\"binary_crossentropy\",metrics=['binary_accuracy', 'mae'])\n","b_model.summary()\n"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1658396651710,"user":{"displayName":"William Lin","userId":"09961641798097151813"},"user_tz":420},"id":"3G1x4z66euwK"},"outputs":[],"source":["# Save checkpoints\n","\n","weights = \"weights/{}_weights.best/hdf5\".format(\"modelbeta\")\n","\n","checkpoint = ModelCheckpoint(\n","    weights, monitor=\"val_loss\", \n","    verbose = 50, \n","    save_best_only = True,\n","    mode = \"min\", \n","    save_weights_only = True  \n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cs9-QRU-Nw3t","outputId":"d66da767-031f-441b-c752-e559d8275586"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2715 - binary_accuracy: 0.9172 - mae: 0.1531\n","Epoch 1: val_loss improved from inf to 0.25639, saving model to weights/modelbeta_weights.best/hdf5\n","938/938 [==============================] - 1073s 1s/step - loss: 0.2715 - binary_accuracy: 0.9172 - mae: 0.1531 - val_loss: 0.2564 - val_binary_accuracy: 0.9188 - val_mae: 0.1369\n","Epoch 2/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2614 - binary_accuracy: 0.9187 - mae: 0.1435\n","Epoch 2: val_loss improved from 0.25639 to 0.25598, saving model to weights/modelbeta_weights.best/hdf5\n","938/938 [==============================] - 1088s 1s/step - loss: 0.2614 - binary_accuracy: 0.9187 - mae: 0.1435 - val_loss: 0.2560 - val_binary_accuracy: 0.9188 - val_mae: 0.1390\n","Epoch 3/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2602 - binary_accuracy: 0.9188 - mae: 0.1430\n","Epoch 3: val_loss improved from 0.25598 to 0.25571, saving model to weights/modelbeta_weights.best/hdf5\n","938/938 [==============================] - 1091s 1s/step - loss: 0.2602 - binary_accuracy: 0.9188 - mae: 0.1430 - val_loss: 0.2557 - val_binary_accuracy: 0.9188 - val_mae: 0.1385\n","Epoch 4/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2592 - binary_accuracy: 0.9188 - mae: 0.1426\n","Epoch 4: val_loss improved from 0.25571 to 0.25562, saving model to weights/modelbeta_weights.best/hdf5\n","938/938 [==============================] - 1169s 1s/step - loss: 0.2592 - binary_accuracy: 0.9188 - mae: 0.1426 - val_loss: 0.2556 - val_binary_accuracy: 0.9188 - val_mae: 0.1380\n","Epoch 5/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2586 - binary_accuracy: 0.9188 - mae: 0.1423\n","Epoch 5: val_loss improved from 0.25562 to 0.25534, saving model to weights/modelbeta_weights.best/hdf5\n","938/938 [==============================] - 1169s 1s/step - loss: 0.2586 - binary_accuracy: 0.9188 - mae: 0.1423 - val_loss: 0.2553 - val_binary_accuracy: 0.9188 - val_mae: 0.1387\n","Epoch 6/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2580 - binary_accuracy: 0.9188 - mae: 0.1420\n","Epoch 6: val_loss improved from 0.25534 to 0.25522, saving model to weights/modelbeta_weights.best/hdf5\n","938/938 [==============================] - 1199s 1s/step - loss: 0.2580 - binary_accuracy: 0.9188 - mae: 0.1420 - val_loss: 0.2552 - val_binary_accuracy: 0.9188 - val_mae: 0.1380\n","Epoch 7/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2578 - binary_accuracy: 0.9188 - mae: 0.1420\n","Epoch 7: val_loss improved from 0.25522 to 0.25503, saving model to weights/modelbeta_weights.best/hdf5\n","938/938 [==============================] - 1205s 1s/step - loss: 0.2578 - binary_accuracy: 0.9188 - mae: 0.1420 - val_loss: 0.2550 - val_binary_accuracy: 0.9188 - val_mae: 0.1386\n","Epoch 8/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2573 - binary_accuracy: 0.9188 - mae: 0.1417\n","Epoch 8: val_loss improved from 0.25503 to 0.25497, saving model to weights/modelbeta_weights.best/hdf5\n","938/938 [==============================] - 1194s 1s/step - loss: 0.2573 - binary_accuracy: 0.9188 - mae: 0.1417 - val_loss: 0.2550 - val_binary_accuracy: 0.9188 - val_mae: 0.1368\n","Epoch 9/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2572 - binary_accuracy: 0.9188 - mae: 0.1416\n","Epoch 9: val_loss improved from 0.25497 to 0.25470, saving model to weights/modelbeta_weights.best/hdf5\n","938/938 [==============================] - 1199s 1s/step - loss: 0.2572 - binary_accuracy: 0.9188 - mae: 0.1416 - val_loss: 0.2547 - val_binary_accuracy: 0.9188 - val_mae: 0.1379\n","Epoch 10/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2570 - binary_accuracy: 0.9188 - mae: 0.1416\n","Epoch 10: val_loss improved from 0.25470 to 0.25445, saving model to weights/modelbeta_weights.best/hdf5\n","938/938 [==============================] - 1201s 1s/step - loss: 0.2570 - binary_accuracy: 0.9188 - mae: 0.1416 - val_loss: 0.2544 - val_binary_accuracy: 0.9188 - val_mae: 0.1403\n","Epoch 11/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2564 - binary_accuracy: 0.9188 - mae: 0.1413\n","Epoch 11: val_loss did not improve from 0.25445\n","938/938 [==============================] - 1195s 1s/step - loss: 0.2564 - binary_accuracy: 0.9188 - mae: 0.1413 - val_loss: 0.2545 - val_binary_accuracy: 0.9188 - val_mae: 0.1358\n","Epoch 12/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2563 - binary_accuracy: 0.9188 - mae: 0.1413\n","Epoch 12: val_loss did not improve from 0.25445\n","938/938 [==============================] - 1204s 1s/step - loss: 0.2563 - binary_accuracy: 0.9188 - mae: 0.1413 - val_loss: 0.2547 - val_binary_accuracy: 0.9188 - val_mae: 0.1339\n","Epoch 13/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2561 - binary_accuracy: 0.9188 - mae: 0.1410\n","Epoch 13: val_loss improved from 0.25445 to 0.25442, saving model to weights/modelbeta_weights.best/hdf5\n","938/938 [==============================] - 1208s 1s/step - loss: 0.2561 - binary_accuracy: 0.9188 - mae: 0.1410 - val_loss: 0.2544 - val_binary_accuracy: 0.9188 - val_mae: 0.1342\n","Epoch 14/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2556 - binary_accuracy: 0.9188 - mae: 0.1409\n","Epoch 14: val_loss improved from 0.25442 to 0.25379, saving model to weights/modelbeta_weights.best/hdf5\n","938/938 [==============================] - 1181s 1s/step - loss: 0.2556 - binary_accuracy: 0.9188 - mae: 0.1409 - val_loss: 0.2538 - val_binary_accuracy: 0.9188 - val_mae: 0.1387\n","Epoch 15/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2557 - binary_accuracy: 0.9188 - mae: 0.1409\n","Epoch 15: val_loss did not improve from 0.25379\n","938/938 [==============================] - 1158s 1s/step - loss: 0.2557 - binary_accuracy: 0.9188 - mae: 0.1409 - val_loss: 0.2540 - val_binary_accuracy: 0.9188 - val_mae: 0.1350\n","Epoch 16/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2554 - binary_accuracy: 0.9188 - mae: 0.1408\n","Epoch 16: val_loss improved from 0.25379 to 0.25364, saving model to weights/modelbeta_weights.best/hdf5\n","938/938 [==============================] - 1156s 1s/step - loss: 0.2554 - binary_accuracy: 0.9188 - mae: 0.1408 - val_loss: 0.2536 - val_binary_accuracy: 0.9188 - val_mae: 0.1365\n","Epoch 17/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2552 - binary_accuracy: 0.9188 - mae: 0.1407\n","Epoch 17: val_loss did not improve from 0.25364\n","938/938 [==============================] - 1161s 1s/step - loss: 0.2552 - binary_accuracy: 0.9188 - mae: 0.1407 - val_loss: 0.2537 - val_binary_accuracy: 0.9188 - val_mae: 0.1346\n","Epoch 18/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2554 - binary_accuracy: 0.9188 - mae: 0.1407\n","Epoch 18: val_loss improved from 0.25364 to 0.25351, saving model to weights/modelbeta_weights.best/hdf5\n","938/938 [==============================] - 1307s 1s/step - loss: 0.2554 - binary_accuracy: 0.9188 - mae: 0.1407 - val_loss: 0.2535 - val_binary_accuracy: 0.9188 - val_mae: 0.1353\n","Epoch 19/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2551 - binary_accuracy: 0.9188 - mae: 0.1406\n","Epoch 19: val_loss improved from 0.25351 to 0.25346, saving model to weights/modelbeta_weights.best/hdf5\n","938/938 [==============================] - 1286s 1s/step - loss: 0.2551 - binary_accuracy: 0.9188 - mae: 0.1406 - val_loss: 0.2535 - val_binary_accuracy: 0.9188 - val_mae: 0.1348\n","Epoch 20/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2547 - binary_accuracy: 0.9188 - mae: 0.1404\n","Epoch 20: val_loss improved from 0.25346 to 0.25327, saving model to weights/modelbeta_weights.best/hdf5\n","938/938 [==============================] - 1268s 1s/step - loss: 0.2547 - binary_accuracy: 0.9188 - mae: 0.1404 - val_loss: 0.2533 - val_binary_accuracy: 0.9188 - val_mae: 0.1350\n","Epoch 21/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2547 - binary_accuracy: 0.9188 - mae: 0.1405\n","Epoch 21: val_loss improved from 0.25327 to 0.25309, saving model to weights/modelbeta_weights.best/hdf5\n","938/938 [==============================] - 1295s 1s/step - loss: 0.2547 - binary_accuracy: 0.9188 - mae: 0.1405 - val_loss: 0.2531 - val_binary_accuracy: 0.9188 - val_mae: 0.1353\n","Epoch 22/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2547 - binary_accuracy: 0.9188 - mae: 0.1404\n","Epoch 22: val_loss improved from 0.25309 to 0.25290, saving model to weights/modelbeta_weights.best/hdf5\n","938/938 [==============================] - 1289s 1s/step - loss: 0.2547 - binary_accuracy: 0.9188 - mae: 0.1404 - val_loss: 0.2529 - val_binary_accuracy: 0.9188 - val_mae: 0.1356\n","Epoch 23/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2544 - binary_accuracy: 0.9188 - mae: 0.1402\n","Epoch 23: val_loss did not improve from 0.25290\n","938/938 [==============================] - 1269s 1s/step - loss: 0.2544 - binary_accuracy: 0.9188 - mae: 0.1402 - val_loss: 0.2532 - val_binary_accuracy: 0.9188 - val_mae: 0.1335\n","Epoch 24/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2543 - binary_accuracy: 0.9188 - mae: 0.1402\n","Epoch 24: val_loss improved from 0.25290 to 0.25267, saving model to weights/modelbeta_weights.best/hdf5\n","938/938 [==============================] - 1273s 1s/step - loss: 0.2543 - binary_accuracy: 0.9188 - mae: 0.1402 - val_loss: 0.2527 - val_binary_accuracy: 0.9188 - val_mae: 0.1360\n","Epoch 25/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2543 - binary_accuracy: 0.9188 - mae: 0.1402\n","Epoch 25: val_loss improved from 0.25267 to 0.25244, saving model to weights/modelbeta_weights.best/hdf5\n","938/938 [==============================] - 1269s 1s/step - loss: 0.2543 - binary_accuracy: 0.9188 - mae: 0.1402 - val_loss: 0.2524 - val_binary_accuracy: 0.9188 - val_mae: 0.1375\n","Epoch 26/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2542 - binary_accuracy: 0.9188 - mae: 0.1401\n","Epoch 26: val_loss did not improve from 0.25244\n","938/938 [==============================] - 1260s 1s/step - loss: 0.2542 - binary_accuracy: 0.9188 - mae: 0.1401 - val_loss: 0.2529 - val_binary_accuracy: 0.9188 - val_mae: 0.1333\n","Epoch 27/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2540 - binary_accuracy: 0.9188 - mae: 0.1400\n","Epoch 27: val_loss improved from 0.25244 to 0.25231, saving model to weights/modelbeta_weights.best/hdf5\n","938/938 [==============================] - 1293s 1s/step - loss: 0.2540 - binary_accuracy: 0.9188 - mae: 0.1400 - val_loss: 0.2523 - val_binary_accuracy: 0.9188 - val_mae: 0.1365\n","Epoch 28/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2539 - binary_accuracy: 0.9188 - mae: 0.1400\n","Epoch 28: val_loss did not improve from 0.25231\n","938/938 [==============================] - 1293s 1s/step - loss: 0.2539 - binary_accuracy: 0.9188 - mae: 0.1400 - val_loss: 0.2525 - val_binary_accuracy: 0.9188 - val_mae: 0.1339\n","Epoch 29/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2538 - binary_accuracy: 0.9188 - mae: 0.1399\n","Epoch 29: val_loss improved from 0.25231 to 0.25224, saving model to weights/modelbeta_weights.best/hdf5\n","938/938 [==============================] - 1262s 1s/step - loss: 0.2538 - binary_accuracy: 0.9188 - mae: 0.1399 - val_loss: 0.2522 - val_binary_accuracy: 0.9188 - val_mae: 0.1353\n","Epoch 30/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2536 - binary_accuracy: 0.9188 - mae: 0.1398\n","Epoch 30: val_loss improved from 0.25224 to 0.25193, saving model to weights/modelbeta_weights.best/hdf5\n","938/938 [==============================] - 1268s 1s/step - loss: 0.2536 - binary_accuracy: 0.9188 - mae: 0.1398 - val_loss: 0.2519 - val_binary_accuracy: 0.9188 - val_mae: 0.1366\n","Epoch 31/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2538 - binary_accuracy: 0.9188 - mae: 0.1400\n","Epoch 31: val_loss did not improve from 0.25193\n","938/938 [==============================] - 1264s 1s/step - loss: 0.2538 - binary_accuracy: 0.9188 - mae: 0.1400 - val_loss: 0.2526 - val_binary_accuracy: 0.9188 - val_mae: 0.1318\n","Epoch 32/50\n","938/938 [==============================] - ETA: 0s - loss: 0.2536 - binary_accuracy: 0.9188 - mae: 0.1399\n","Epoch 32: val_loss did not improve from 0.25193\n","938/938 [==============================] - 1268s 1s/step - loss: 0.2536 - binary_accuracy: 0.9188 - mae: 0.1399 - val_loss: 0.2521 - val_binary_accuracy: 0.9188 - val_mae: 0.1344\n","Epoch 33/50\n","653/938 [===================>..........] - ETA: 4:51 - loss: 0.2546 - binary_accuracy: 0.9182 - mae: 0.1404"]}],"source":["# Train the model \n","b_model.fit(train_ds,\n","  epochs=1,\n","  validation_data = val_ds, \n","  callbacks = checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f8FcEkcdeuwK"},"outputs":[],"source":["# Fit and train use fit_generator() to fit in the batches of images \n","\n","\"\"\"\n","b_model.fit_generator(\n","    train_generator, \n","    steps_per_epoch = train_generator.n//train_generator.batch_size, \n","    validation_data = val_generator,\n","    validation_steps = val_generator.n//val_generator.batch_size,\n","    epochs = 1,\n","    callbacks = checkpoint \n",")\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"ZV_50qn8euwK"},"source":["### Model Predictions\n","\n","- The model was tested on 10,000 images separate from the train and val sets. \n","\n","- ImageDataGenerator was used here to generate the batches of images for the testing set as previous method requires lables to be present. I have not refactored the code to be suitable for the test set. \n","\n","- The model outputs probailities across different labels. \n","\n","- Single image predictions will be performed to assess classifications and for further deployment to the web app. This will be done in a separate file. "]},{"cell_type":"markdown","metadata":{},"source":["See the Avg Distribution of labels in the current test_set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XA0cDzRNeuwL"},"outputs":[],"source":["# Transform the labels into a list of binaries\n","test_Y = test_set.apply(lambda x: [x[labels].values], axis = 1).map(lambda x: x[0])\n","\n","# See the average distribution of the true labels in the test set \n","for c_label, s_count in zip(labels, 100*np.mean(test_Y,0)):\n","    print('%s: %2.2f%%' % (c_label, s_count))"]},{"cell_type":"markdown","metadata":{},"source":["Predict using Test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-AMNjQD_fIU9"},"outputs":[],"source":["# Instantiate ImageDataGenerator instance only for the test data\n","imdg = ImageDataGenerator(rescale = 1/255)\n","\n","# Create a generator for the test images since the custom function requies labels \n","\n","test_generator = imdg.flow_from_dataframe(\n","    dataframe=test_set,\n","    x_col=\"path\",\n","    target_size=(224, 224),\n","    color_mode=\"rgb\",\n","    batch_size=25,\n","    class_mode=None,\n","    shuffle=False\n",")\n","\n","# Predict the Test Dataset \n","pred_y = b_model.predict(\n","    test_generator, \n","    steps = test_generator.n//test_generator.batch_size,\n","    verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m1EvlKDseuwM"},"outputs":[],"source":["# Transform the binarized list of labels to a numpy array or else a for loop will not be able to index well\n","nptest_Y = test_set[labels].to_numpy(dtype = int)\n","nptest_Y.shape"]},{"cell_type":"markdown","metadata":{},"source":["### Model Evaluation and Visualizations\n","\n","- The predictions were visually evaluated using the AUC/ROC curve and confusion matrices for each individual labels. Both of these visualizations evaluate the differences between the model's predictions of True/False Positives and True/False Negatives. \n","\n","- The [AUC/ROC curve](https://github.com/emilybstevens/CXR-ML/blob/main/Resources/betanet/beta_1epoch.png) for the current working model is availabel in the repo. \n","    \n","    - Higher AUC, the better the model is at predicting a specific class label. The ROC Curve displays the model's classification accuracy across each label based on the true positives and false positives. Ideally, the curves for all labels should be veering towards the top left corner since we want to maximize the true positive and minimize the false positive values. \n","\n","- Each label's [confusion matrices](https://github.com/emilybstevens/CXR-ML/tree/main/Resources/betanet) are available as png images beginning with `cm`. \n","\n","- Based on the performance of the current model, it is evident that image augmentation plays a very important role in the accuracy of the model. The confusion matrices indicate that the model is capable of predicting true negatives well but not true positives. Some labels do better than others. This could mean:\n","    \n","    - The features are not being picked out well by the extractor layer due to lack of augmentation. \n","    - The model should only be trained up to 10 epochs or there is a risk of overtraining for feature identification. \n","    - The sample size distribution should include more no_finding images to the model can generalize the classifications better. "]},{"cell_type":"markdown","metadata":{},"source":["AUC/ROC Curve"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WSmWQHPBeuwM"},"outputs":[],"source":["# Create an AUC / ROC curve \n","fig, c_ax = plt.subplots(1,1, figsize = (15, 15))\n","\n","# Loop through all the labels and get the index and values. \n","for (idx, c_label) in enumerate(labels):\n","    # create variables for the roc curve based on its output.\n","    #The for loop loops through all the values in the list of binarized labels based on index via column of the labels\n","    fpr, tpr, thresholds = roc_curve(nptest_Y[:,idx].astype(int), pred_y[:,idx])\n","    c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n","c_ax.legend()\n","c_ax.set_xlabel('False Positive Rate')\n","c_ax.set_ylabel('True Positive Rate')\n","fig.savefig('beta_50epochs.png')"]},{"cell_type":"markdown","metadata":{},"source":["Save the outputs that are greater than 20% probability as a csv file of predicted labels "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9QTlGgureuwM"},"outputs":[],"source":["# See Predictions greater than 20% for the provisional model and save to csv\n","predictions = (pred_y > 0.2).astype(int)\n","columns=labels\n","#columns should be the same order of y_col\n","results=pd.DataFrame(predictions, columns=columns)\n","results[\"Filenames\"]=test_set[\"Image Index\"].tolist()\n","ordered_cols=[\"Filenames\"]+columns\n","results=results[ordered_cols]#To get the same column order\n","results.to_csv('beta_net_predictions.csv')\n","\n","print(results)"]},{"cell_type":"markdown","metadata":{},"source":["Confusion Matrices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xUOzYigzsD2-"},"outputs":[],"source":["# Import Dependencies \n","from sklearn.metrics import multilabel_confusion_matrix, ConfusionMatrixDisplay\n","\n","# Create the confusion matrix for each of the labels \n","conf_mat = multilabel_confusion_matrix(y_true = nptest_Y, y_pred= np.array(results[labels], dtype = int))\n","\n","# Plot the individual confusion matrices for the test sample. \n","\n","for i in range(len(conf_mat)):\n","  disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat[i])\n","  disp.plot()\n","  plt.title(labels[i])\n","  plt.savefig(f\"cm_{labels[i]}\")\n","  plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Save the Model "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XPZuiKNHeuwN"},"outputs":[],"source":["# Required to save models in HDF5 format\n","!pip install pyyaml h5py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SDz7P9apoEJV"},"outputs":[],"source":["# Save the entire model to a HDF5 file.\n","# The '.h5' extension indicates that the model should be saved to HDF5.\n","b_model.save('b_model.h5') "]},{"cell_type":"markdown","metadata":{},"source":["Please feel free to write me comments ici:\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"beta_net.ipynb","provenance":[]},"gpuClass":"standard","interpreter":{"hash":"147be817500740479c9742981730847e5eff136c2e0bec224523841b37ad5343"},"kernelspec":{"display_name":"Python 3.7.13 ('mlenv')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"vscode":{"interpreter":{"hash":"5c1fa0b4963365e064c71d8bf73c6819fefc3ad2c3fc999bb7f7dad28f1ba257"}}},"nbformat":4,"nbformat_minor":0}
